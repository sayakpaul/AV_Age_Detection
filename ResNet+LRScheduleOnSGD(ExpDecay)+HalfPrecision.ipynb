{
  "cells": [
    {
      "metadata": {
        "_uuid": "c52e920abf0758542f1d428d199dc1df6851ed4e"
      },
      "cell_type": "markdown",
      "source": "This problem is taken from this [https://datahack.analyticsvidhya.com/contest/practice-problem-age-detection/](https://datahack.analyticsvidhya.com/contest/practice-problem-age-detection/). \n\n>The task is to predict the age of a person from his or her facial attributes. For simplicity, the problem has been converted to a multi-class problem with classes as Young, Middle and Old.\n\nI attempted this problem almost 1.5 years ago with only the knowledge of Convolutions. \n\nToday, I am at least in a position to apply Deep Neural Networks to problems like these. Thanks to the community and great learning resources. The code that is presented here have been taken from many places. I think being a programmer it is equally important to be able read people's code and reuse it. (Opinions are mine)\n\nI would like to thank the following people specifically - \n- Adrian Rosebrock, for putting together a tutorial on [Deep Learning and Medical Image Analysis with Keras](https://www.pyimagesearch.com/2018/12/03/deep-learning-and-medical-image-analysis-with-keras/) which is one of the classiest tutorials I have ever read. \n- FAIZAN SHAIKH, for the blog [Hands on with Deep Learning â€“ Solution for Age Detection Practice Problem](https://www.analyticsvidhya.com/blog/2017/06/hands-on-with-deep-learning-solution-for-age-detection-practice-problem/) which tremendously served me as a reference. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0f6ea66f5d5c0019b3f03260220a8a9c54de02a1"
      },
      "cell_type": "code",
      "source": "# Dependencies\n\nimport os\nimport random\nimport numpy as np \n\nimport pandas as pd\nfrom scipy.misc import imread\nfrom subprocess import check_output\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c1902064832197d6641ab62ca7cb2e198b54acbc"
      },
      "cell_type": "code",
      "source": "# i used Adrian's custom ResNet model. For importing modules from custom scripts in Kaggle\n# Kernels, you need to do some hacks. The following ensures the initial directory \n# structure is retained if anything went wrong in order to load up the custom script. \nos.chdir(\"/kaggle/working/\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "66298544ee60bc2dc9c1010c66b54243f15ed47c"
      },
      "cell_type": "code",
      "source": "train_csv = pd.read_csv('../input/traincsv/train.csv')\ntest_csv = pd.read_csv('../input/testcsv/test.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eb282e5aaaf9755c6f25967f43f374018d2b9f97",
        "_kg_hide-output": true
      },
      "cell_type": "code",
      "source": "print(check_output([\"ls\", \"../input/traintestzip/train/Train\"]).decode(\"UTF-8\"))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a8fdbc91d04b7504d0612bb1658140fa123d5188",
        "_kg_hide-output": true
      },
      "cell_type": "code",
      "source": "print(check_output([\"ls\", \"../input/traintestzip/test/Test\"]).decode(\"UTF-8\"))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6475542eb71aec55bd550125eacb57672f3ba109",
        "_kg_hide-output": true
      },
      "cell_type": "code",
      "source": "# Load up a random image and display it. Along with it display the age of the person present in the image.\n# Thanks, Faizan. \nfrom scipy.misc import imshow\nimport matplotlib.pyplot as plt\ni = random.choice(train_csv.index)\n\nimg_name = train_csv.ID[i]\nimg = imread(os.path.join('../input/traintestzip/train', 'Train', img_name))\n\n\n#imshow(img)\nplt.imshow(img)\nplt.show()\nprint(\"Age:\" + train_csv.Class[i])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "231228a9cdcabc62cb066d18535cbaeadabd0433"
      },
      "cell_type": "markdown",
      "source": "# Resizing the images fron train and test set to 64 * 64 and half precision (float16).\n> From my experiments, I saw that half-precision policy gave a tremendous speed boost and an improved performance score. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "098745273352e31589692410c82285c21d7dbad2"
      },
      "cell_type": "code",
      "source": "from scipy.misc import imresize\nimport numpy as np\n\ntemp = []\nfor img_name in train_csv.ID:\n    img_path = os.path.join('../input/traintestzip/train', 'Train', img_name)\n    img = imread(img_path)#, flatten=True) # Remove Greyscaling\n    img = imresize(img, (64, 64))\n    img = img.astype('float16') # Changed the precision to 64, resource problem not done\n    temp.append(img)\n\ntrain_x = np.stack(temp)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0ab2692d0297d94e2b6637f17b351cb183d98d8b"
      },
      "cell_type": "code",
      "source": "temp = []\nfor img_name in test_csv.ID:\n    img_path = os.path.join('../input/traintestzip/test', 'Test', img_name)\n    img = imread(img_path)#, flatten = True)\n    img = imresize(img, (64, 64))\n    temp.append(img.astype('float16'))\n\ntest_x = np.stack(temp)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3624d71f2108bed78c38100b1beaf38acd02ee78"
      },
      "cell_type": "code",
      "source": "# # Required if greyscaling is applied\n# train_x = np.expand_dims(train_x, axis=3)\n# test_x = np.expand_dims(test_x, axis=3)\n# train_x.shape, test_x.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "822feb11b6d8a6779b8e98cacd2157662655c127"
      },
      "cell_type": "markdown",
      "source": "# PyImageSearch Patches"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "05b954c951450e60d37b625c3ebf2270045cd92a",
        "_kg_hide-output": false
      },
      "cell_type": "code",
      "source": "!pip install --upgrade imutils",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "beb1636fff55978bccb953df70f616136bf1dc6f"
      },
      "cell_type": "markdown",
      "source": "**imutils** is a utility class written by Adrian which provides many convenience functions required during Image Processing tasks. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2006ed43976e30e68ea8ded1d69861bc88937f8e",
        "_kg_hide-output": true
      },
      "cell_type": "code",
      "source": "from imutils import paths\n# determine the total number of image paths in training, validation,\n# and testing directories\ntotalTrain = len(list(paths.list_images(os.path.join('../input/traintestzip/train', 'Train'))))\n#totalVal = len(list(paths.list_images(config.VAL_PATH)))\ntotalTest = len(list(paths.list_images(os.path.join('../input/traintestzip/test', 'Test'))))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3e9688f9077065719858d6afdd1f005516726c7c"
      },
      "cell_type": "code",
      "source": "totalTrain, totalTest",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "84974f558859dfc3eb6abe040accbb9e19aecdeb"
      },
      "cell_type": "markdown",
      "source": "## Normalization of the pixels"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a22c7d812f785b081f7f9367471d3e337cad3153"
      },
      "cell_type": "code",
      "source": "train_x = train_x / 255.\ntest_x = test_x / 255.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5d2f8e3d289b70a142aa9e46f899ea638f411f98"
      },
      "cell_type": "markdown",
      "source": "## Label encode the classes"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8ce459ccddc7ac41c2b9cd085e924d321f63ef1b"
      },
      "cell_type": "code",
      "source": "import keras\nfrom sklearn.preprocessing import LabelEncoder\n\nlb = LabelEncoder()\ntrain_y = lb.fit_transform(train_csv.Class)\ntrain_y = keras.utils.np_utils.to_categorical(train_y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "dd9c719e4f776a16d0dfa08ff54517cea89d1bc9"
      },
      "cell_type": "markdown",
      "source": "## Split the training set into further training and test set"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c2883e5d050ad754baaf1e141c2a26c17a052cf2"
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_x, train_y, test_size=0.2, random_state=42)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "910d8fc78f52dfd4bc420e2e9a20ce72a59aed33"
      },
      "cell_type": "code",
      "source": "X_train.shape, X_test.shape, y_train.shape, y_test.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4087eb3354a664dd75c21a1b2dc28bbe39dda4f2"
      },
      "cell_type": "markdown",
      "source": "## Data augmentation for the training set to enhance the training (did not improve the performance much, hence the code for this is commented). "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3efe155c5be33e7793d2d5ebd7d751b5e947ef2c"
      },
      "cell_type": "code",
      "source": "# from keras.preprocessing.image import ImageDataGenerator\n\n# # initialize the training training data augmentation object\n# trainAug = ImageDataGenerator(\n#     featurewise_center=True,\n#     featurewise_std_normalization=True,\n#     rotation_range=20,\n#     width_shift_range=0.2,\n#     height_shift_range=0.2,\n#     horizontal_flip=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "65410cc036d06f10a42bf18159ccf91e3ca7ec4d"
      },
      "cell_type": "code",
      "source": "# trainAug.fit(X_train)\n# trainAug.fit(X_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7755738397de3fb52f259ab4cd4e9c9c9e36d022"
      },
      "cell_type": "markdown",
      "source": "## Setting up the ResNet model using Adrian's code"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0a5a517cfd7f5011e9d21c385c703c3560d195f1"
      },
      "cell_type": "code",
      "source": "os.chdir('../input/pyimagesearch/')\nfrom resnet import ResNet",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "86671e7ba476ddfaf22915c35b9716973a5ff324"
      },
      "cell_type": "markdown",
      "source": "**Defining the Learning rate schedule. A small hack which speeds up the training process. But this can even be enhanced with adaptive learning rates since adaptive LRs eliminate man constraints of LR schedulers. We also define two of the most important hyperparameters required during the training - `batch size` and `epoch`.**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "010d9283f3d7c336e833769409f5010e7265ba24"
      },
      "cell_type": "code",
      "source": "# define the total number of epochs to train for along with the\n# initial learning rate and batch size\nNUM_EPOCHS = 50\nINIT_LR = 1e-1\nBS = 64 #32 is good for CPU\ndef poly_decay(epoch):\n    # initialize the maximum number of epochs, base learning rate,\n    # and power of the polynomial\n    maxEpochs = NUM_EPOCHS\n    baseLR = INIT_LR\n    power = 1.0\n \n# compute the new learning rate based on polynomial decay\n    alpha = baseLR * (1 - (epoch / float(maxEpochs))) ** power\n \n# return the new learning rate\n    return alpha",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "821934f875e61fd683384ed488b2e9fbc6267723"
      },
      "cell_type": "markdown",
      "source": "### Model initialization"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d1bab902dac77f4749d7986dbbea406b55f142e5"
      },
      "cell_type": "code",
      "source": "from keras.optimizers import SGD\n#from keras.optimizers import Adam\n# initialize our ResNet model and compile it\n\n#model = ResNet.build(64, 64, 3, 3, (3, 4, 6), (16, 32, 64, 128), reg=0.0001) # 0.0005\n# We used the above build earlier. Trying a more complex architecture now\n\nmodel = ResNet.build(64, 64, 3, 3, (3, 4, 6), (16, 32, 64, 128), reg=0.0005)\n\n# model = ResNet.build(64, 64, 3, 3, (3, 4, 6),\n# (64, 128, 256, 512), reg=0.0005) # Overfits >_<\n\n# model = ResNet.build(64, 64, 3, 3, (3, 4, 6),\n# (32, 64, 128, 256), reg=0.0005)\n\nopt = SGD(lr=INIT_LR, momentum=0.9)\n#opt = Adam(lr=0.01) # Trying out Adam 0.1\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0538d587867719728144aa2983957fc791240c62"
      },
      "cell_type": "code",
      "source": "# # Set up CLR\n# os.chdir('../input/clr-scripts/')\n\n# from clr_callback import *\n# clr_triangular = CyclicLR(mode='triangular')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "09844b02bef829266cee9a04468c81498d09f093"
      },
      "cell_type": "markdown",
      "source": "### Finally setup the scheduler as a Keras callback and fit the model with it"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2ab50c38440d664556f7deecd77f175ca7daa1fc",
        "_kg_hide-output": true
      },
      "cell_type": "code",
      "source": "# Define our set of callbacks and fit the model\nfrom keras.callbacks import LearningRateScheduler\ncallbacks = [LearningRateScheduler(poly_decay)]\n\n\n\n%time H = model.fit(X_train, y_train, batch_size=BS,epochs=NUM_EPOCHS,verbose=1,\\\n                    validation_data = (X_test, y_test), callbacks=callbacks)\n\n# BS - Epoch tradeoff\n\n# %time H = model.fit(X_train, y_train, batch_size=32,epochs=(len(X_train)//32),verbose=1,\\\n#                     validation_data = (X_test, y_test)) #callbacks=callbacks) # No callbacks since Adam\n\n# %time H = model.fit_generator(trainAug.flow(X_train, y_train, batch_size=BS),\\\n#                               steps_per_epoch=len(X_train) // BS, \\\n#                               validation_data=trainAug.flow(X_test, y_test, batch_size=BS),\\\n#                               validation_steps=len(X_test) // BS,\\\n#                               epochs=NUM_EPOCHS,\\\n#                               callbacks=callbacks)\n\n# Let's try with a larger batch size while keeping the no. of epochs smaller constant\n# %time H = model.fit(X_train, y_train, batch_size=2000,epochs=20,verbose=1,\\\n#                     validation_data = (X_test, y_test), callbacks=[clr_triangular])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "778d55091ad61030f3f845eead4e6c9b875af0a7"
      },
      "cell_type": "markdown",
      "source": "Image augmentation did not help much in this case. So let's try the model without image augmentation. The problem also tells us that image augmentation is not going to help much here. "
    },
    {
      "metadata": {
        "_uuid": "6fe50559d3101b3fed0aae6bbc416b3f329b91cd"
      },
      "cell_type": "markdown",
      "source": "### The model takes approaximately 51.5 minutes to train on Kaggle Kernels.Let's serialize this model in .h5 format for later usage."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5e3f5aceca94c50055569fbab6ad626b0dba4c72"
      },
      "cell_type": "code",
      "source": "os.chdir(\"/kaggle/working/\")\nmodel.save('model_resnet.h5')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "251550deae390c486cd6b9364ee272dcd0dff919"
      },
      "cell_type": "code",
      "source": "!ls -l --block-size=M",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "34071a58730c7d0291c0395e02f0c7caee62a90d"
      },
      "cell_type": "markdown",
      "source": "### A chart for judging the model."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1f851501f362e88714a8e5c351fcf55478554314"
      },
      "cell_type": "code",
      "source": "# Plot the training loss and accuracy.\nimport matplotlib.pyplot as plt\n \n# Plot\nN = NUM_EPOCHS\nplt.style.use(\"ggplot\")\nplt.figure()\nplt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\nplt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\nplt.plot(np.arange(0, N), H.history[\"acc\"], label=\"train_acc\")\nplt.plot(np.arange(0, N), H.history[\"val_acc\"], label=\"val_acc\")\nplt.title(\"Training Loss and Accuracy on Dataset\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss/Accuracy\")\nplt.legend(loc=\"lower left\")\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bb973e01fbfec8044806f2a47bfa1e4107753281"
      },
      "cell_type": "markdown",
      "source": "**Overfitting due to the increased complexity of the model. Hence, let's try a lesser complex model and see.  Accuracy is not improving much with increased complexities. Let's get back to the baseline and try with greyscaled images.**"
    },
    {
      "metadata": {
        "_uuid": "2a0b62c254ca2b6d9f9904f2c3beb7f06baa7283"
      },
      "cell_type": "markdown",
      "source": "### Make predicitions on the original test data i.e. `test_x`  and prepare the submission file"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "57f5e8562d99b21d47ecd82f8500e429e8ea3e17"
      },
      "cell_type": "code",
      "source": "pred = model.predict_on_batch(test_x)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "26c280986f7e4ed8a0bf2762851ca17c3d0662a3"
      },
      "cell_type": "code",
      "source": "pred",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cfa2c98cf9514ddee47920d6a72c7bb8d6887715"
      },
      "cell_type": "code",
      "source": "# for each image in the testing set we need to find the index of the\n# label with corresponding largest predicted probability\npredIdxs = np.argmax(pred, axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c03e8bad8921acdeaa16d3f67e2f6cc16adb3860"
      },
      "cell_type": "code",
      "source": "pred_transform = lb.inverse_transform(predIdxs) # Transform labels back to original encoding.\ntest_csv['Class'] = pred_transform\ntest_csv.to_csv('submission_sayak_resnet.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e06c3e184cdc0c30d458a5f551960ecfe71aa5c7"
      },
      "cell_type": "code",
      "source": "!head -5 submission_sayak_resnet.csv",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}